# ARBITRARY-STYLE-TRANSFER
This paper focuses on arbitrary style transfer using machine learning to apply the visual style of
one image onto another while maintaining the original content. The project aims to develop an
efficient and flexible deep-learning model for style transfer, emphasizing distortion reduction. The
methodology involves dataset collection, preprocessing, and CNN-based algorithm implementation
for universal style transfer. The model is trained to learn diverse styles for transforming ordinary
images into artistic renditions. The project begins with a VGG-19 pre-trained model on ImageNet,
using the Gram matrix for style blending. To overcome VGG-19 limitations, the ResNet-34 model
is explored, and the paper details its implementation, including Gram matrix utilization, Loss and
Weights Calculations, and outcomes. The results highlight advancements in arbitrary style transfer,
addressing challenges in pre-existing models and suggesting future avenues for deep learning and
image processing exploration.
